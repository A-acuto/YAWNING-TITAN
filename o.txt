============================= test session starts =============================
platform win32 -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0
rootdir: D:\Pycharm projects\YAWNING-TITAN-DEV\YAWNING-TITAN, configfile: pytest.ini
collected 71 items / 70 deselected / 1 selected

tests\test_red_target_specific_node.py Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Eval num_timesteps=1000, episode_reward=-134.42 +/- 11.28
Episode length: 12.80 +/- 2.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.8     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-128.31 +/- 4.82
Episode length: 9.60 +/- 1.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.6      |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 13       |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 422      |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 2048     |
---------------------------------
['0', '5', '5', '5', '7', '7', '8', '8', '7', '8', '8', '9', '8', '8', '8', '8', '9', '9', '9', '9', '0', '5', '5', '5', '5', '5', '7', '7', '8', '8', '8', '8', '8', '9', '0', '5', '5', '7', '8', '8', '9', '9']
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Eval num_timesteps=1000, episode_reward=-126.12 +/- 7.15
Episode length: 10.00 +/- 2.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-124.46 +/- 7.23
Episode length: 9.00 +/- 4.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 11.8     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 438      |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 2048     |
---------------------------------
['0', '5', '7', '5', '7', '7', '8', '8', '8', '8', '8', '9', '0', '5', '5', '7', '8', '8', '9', '9', '8', '8', '9', '0', '5', '7', '7', '7', '8', '9']
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Eval num_timesteps=1000, episode_reward=-162.24 +/- 9.71
Episode length: 21.00 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-135.19 +/- 19.07
Episode length: 14.40 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.3     |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 323      |
|    iterations      | 1        |
|    time_elapsed    | 6        |
|    total_timesteps | 2048     |
---------------------------------
['0', '5', '5', '5', '5', '5', '5', '0', '0', '5', '5', '7', '7', '7', '7', '8', '9', '9', '0', '5', '5', '5', '5', '7', '8', '8', '8', '8', '8', '9', '9', '9', '9', '9', '0', '5', '5', '5', '5', '5', '7', '7', '7', '8', '8', '9', '9', '9']
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Eval num_timesteps=1000, episode_reward=-130.17 +/- 16.29
Episode length: 12.60 +/- 4.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.6     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-128.69 +/- 14.39
Episode length: 11.20 +/- 2.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.2     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 12.5     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 420      |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 2048     |
---------------------------------
['0', '5', '5', '5', '5', '5', '7', '8', '7', '7', '5', '7', '8', '8', '8', '8', '9', '9', '9', '0', '5', '7', '7', '7', '7', '8', '8', '8', '8', '8', '9', '9', '9', '9', '9', '0', '5', '7', '8', '8', '8', '9', '9', '9']
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Eval num_timesteps=1000, episode_reward=-132.64 +/- 11.51
Episode length: 12.80 +/- 3.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.8     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-133.42 +/- 4.33
Episode length: 11.80 +/- 2.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.8     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 13.2     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 437      |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 2048     |
---------------------------------
['0', '5', '7', '8', '9', '0', '5', '5', '5', '7', '7', '8', '8', '8', '8', '9', '0', '5', '7', '7', '7', '8', '9', '9', '9', '9']
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Eval num_timesteps=1000, episode_reward=-131.31 +/- 9.20
Episode length: 11.40 +/- 3.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 11.4     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-142.14 +/- 14.26
Episode length: 16.80 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 13.4     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 444      |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 2048     |
---------------------------------
['0', '5', '0', '0', '0', '5', '7', '7', '8', '9', '9', '0', '5', '7', '7', '7', '7', '7', '8', '8', '8', '8', '9', '9', '9', '0', '5', '5', '5', '7', '7', '8', '9', '9']
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Eval num_timesteps=1000, episode_reward=-126.92 +/- 12.93
Episode length: 10.60 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10.6     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-136.74 +/- 9.97
Episode length: 14.20 +/- 3.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 13.1     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 393      |
|    iterations      | 1        |
|    time_elapsed    | 5        |
|    total_timesteps | 2048     |
---------------------------------
['0', '5', '5', '5', '7', '7', '8', '9', '9', '9', '9', '0', '5', '5', '5', '5', '7', '8', '8', '9', '9', '9', '0', '5', '5', '7', '8', '8', '9', '9']
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Eval num_timesteps=1000, episode_reward=-134.35 +/- 13.89
Episode length: 12.80 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.8     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-135.90 +/- 10.29
Episode length: 13.80 +/- 4.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 12.9     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 454      |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 2048     |
---------------------------------
['0', '5', '5', '5', '5', '5', '7', '8', '8', '8', '8', '9', '9', '9', '9', '0', '0', '0', '0', '5', '5', '7', '7', '7', '7', '7', '8', '7', '7', '7', '8', '9', '9', '0', '0', '0', '0', '0', '0', '5', '5', '7', '7', '8', '9', '9', '9', '9', '9']
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Eval num_timesteps=1000, episode_reward=-136.94 +/- 26.76
Episode length: 12.00 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12       |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-141.74 +/- 7.29
Episode length: 15.40 +/- 1.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 13       |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 444      |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 2048     |
---------------------------------
['0', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '7', '5', '5', '5', '7', '7', '7', '7', '7', '8', '8', '9', '9', '9', '0', '5', '7', '8', '8', '8', '9', '9', '9', '9', '0', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '7', '8', '9', '9']
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Eval num_timesteps=1000, episode_reward=-130.91 +/- 6.77
Episode length: 12.40 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.4     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-139.20 +/- 6.80
Episode length: 12.40 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.4     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 12.6     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 439      |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 2048     |
---------------------------------
['0', '5', '5', '7', '7', '8', '9', '9', '9', '0', '5', '7', '8', '8', '8', '9', '0', '5', '7', '7', '7', '8', '9', '9', '9']
{'7', '0', '9', '5', '8'}
.

============================== warnings summary ===============================
yawning_titan\envs\generic\core\network_interface.py:502: 13 warnings
  D:\Pycharm projects\YAWNING-TITAN-DEV\YAWNING-TITAN\yawning_titan\envs\generic\core\network_interface.py:502: DeprecationWarning: Sampling from a set deprecated
  since Python 3.9 and will be removed in a subsequent version.
    self.high_value_nodes = random.sample(

venv\lib\site-packages\gensim\matutils.py:22
  D:\Pycharm projects\YAWNING-TITAN-DEV\YAWNING-TITAN\venv\lib\site-packages\gensim\matutils.py:22: DeprecationWarning: Please use `triu` from the `scipy.linalg` namespace, the `scipy.linalg.special_matrices` namespace is deprecated.
    from scipy.linalg.special_matrices import triu

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
========== 1 passed, 70 deselected, 14 warnings in 70.97s (0:01:10) ===========
